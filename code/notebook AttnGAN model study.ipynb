{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AttnGAN\n",
    "## Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\n",
    "\n",
    "http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.pdf  \n",
    "https://github.com/taoxugit/AttnGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- run le code en debug dans IntelliJ\n",
    "- indiquer les shapes dans le code de ce notebook\n",
    "- copier et analyser la fonction train DAMSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/AttnGAN-framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **z** - a noise vector usually sampled from a standard normal distribution.  \n",
    "- **F**<sup>*ca*</sup> - represents the Conditioning Augmentation that converts the sentence vector **E** to the conditioning vector.\n",
    "  - **E** is a global sentence vector, and **e** is the matrix of word vectors.\n",
    "- **F**<sub>i</sub><sup>*attn*</sup> is the proposed attention model at the i<sup>*th*</sup> stage of the AttnGAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **F**<sup>*ca*</sup>, **F**<sub>i</sub><sup>*attn*</sup>, **F**<sub>i</sub> , and **G**<sub>i</sub> are neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The attention model **F**<sup>*attn*</sup> has two inputs:  \n",
    "the word features **e** and the image features from the previous hidden layer h.\n",
    "\n",
    "The word features are first converted into the common semantic space of the image features by adding a new perceptron layer.  \n",
    "\n",
    "Then, a word-context vector is computed for each sub-region of the image based on its hidden features **h** (query). Each column of h is a feature vector of a sub-region of the image. For the j<sup>th</sup> sub-region, its word-context vector is a dynamic representation of word vectors relevant to h<sub>j</sub>.  \n",
    "\n",
    "Finally, image features and the corresponding word-context features are combined to generate images at the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Attentional Multimodal Similarity Model (DAMSM)\n",
    "\n",
    "The DAMSM **learns two neural networks** that map sub-regions of the image and words of the sentence to a common semantic space, thus measures the **image-text similarity** at the word level to compute a fine-grained **loss for image generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoder \n",
    "\n",
    "A **bi-directional LSTM** that extracts semantic vectors from the text description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/text_encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word features**: The feature matrix of words indicated by **e**. Its i<sup>*th*</sup> column **e**<sub>i</sub> is feature vector for the i<sup>*th*</sup> word.  \n",
    "\n",
    "**sentence feature**: The last hidden states of the bi-directional LSTM are concatenated to be the global sentence vector, denoted by **E**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_ENCODER(nn.Module):\n",
    "    def __init__(self, ntoken=2932, ninput=300, drop_prob=0.5, nhidden=256, nlayers=1, bidirectional=True):\n",
    "        '''\n",
    "        ntoken  -- size of the dictionary computed from the dataset captions; 2932 tokens in FashionGen2\n",
    "        nhidden -- TEXT.EMBEDDING_DIM = 256 for FashionGen2       \n",
    "        ninput  -- size of each embedding vector (300 by default)\n",
    "        \n",
    "        nlayers -- Number of recurrent layers\n",
    "        '''\n",
    "        self.n_steps = cfg.TEXT.WORDS_NUM  # 10 in FashionGen2 (caption max number of words)\n",
    "        # ...\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "\n",
    "        # number of features in the hidden state (hidden nodes in the LSTM layer)\n",
    "        self.nhidden = nhidden // self.num_directions  # 128 = 256 / 2  (1 Bi-LSTM layer of 128 nodes)\n",
    "\n",
    "    def define_module(self):\n",
    "        # ...\n",
    "        self.encoder = nn.Embedding(self.ntoken, self.ninput)  # nn.Embedding(2932, 300)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
    "                               self.nlayers, batch_first=True,\n",
    "                               dropout=self.drop_prob,\n",
    "                               bidirectional=self.bidirectional)\n",
    "\n",
    "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
    "        # input: torch.LongTensor of size batch x n_steps --> emb: batch x n_steps x ninput\n",
    "        # input (bs, 10) --> emb (bs, 10, 300)\n",
    "        emb = self.drop(self.encoder(captions))\n",
    "        \n",
    "        # Returns: a PackedSequence object\n",
    "        cap_lens = cap_lens.data.tolist()\n",
    "        # cap_lens -> <type 'list'>: [10, 10, 10, 10, 10, 10, 10, 9, 9, 8, 8, 8, 7, 7, 7, \n",
    "        #                              7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5]\n",
    "        \n",
    "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
    "        \n",
    "        # emb -> PackedSequence(tensor([[-0.0112,  0.0000, -0.0002,  ...,  0.5088,  0.0000, -0.1240],\n",
    "        #                               [-0.0000,  0.0782,  0.3802,  ..., -0.3164,  0.4351,  0.0748],\n",
    "        #                               [-0.1120, -0.0000, -0.1069,  ..., -0.0000,  0.0000, -0.0000],\n",
    "        #                               ...,\n",
    "        #                               [ 0.0000, -0.1407, -0.6452,  ...,  0.0000, -0.0000,  0.4360],\n",
    "        #                               [-0.0000, -0.0074,  0.0000,  ...,  0.1719,  0.0000,  0.0082],\n",
    "        #                               [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  2.9084]]\n",
    "    \n",
    "        \n",
    "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
    "        #     tensor containing the initial hidden state for each element in batch.\n",
    "        # #output (batch, seq_len, hidden_size * num_directions) or a PackedSequence object:\n",
    "        #     tensor containing output features (h_t) from the last layer of RNN\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        # PackedSequence object --> (batch, seq_len, hidden_size * num_directions)\n",
    "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
    "        \n",
    "        # output = self.drop(output) --> batch x hidden_size * num_directions x seq_len\n",
    "        words_emb = output.transpose(1, 2)\n",
    "        \n",
    "        # words_emb.shape --> torch.Size([32, 256, 10])\n",
    "        \n",
    "        # --> batch x num_directions * hidden_size\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            sentence_emb = hidden[0].transpose(0, 1).contiguous()\n",
    "            \n",
    "        sentence_emb = sentence_emb.view(-1, self.nhidden * self.num_directions)  # (-1, 128*2)\n",
    "\n",
    "        # words_emb.shape --> torch.Size([32, 256, 10])\n",
    "        # sentence_emb    --> torch.Size([32, 256])\n",
    "        return words_emb, sentence_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_ENCODER(\n",
    "  (encoder): Embedding(2932, 300)\n",
    "  (drop): Dropout(p=0.5)\n",
    "  (rnn): LSTM(300, 128, batch_first=True, dropout=0.5, bidirectional=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretrained **Inception v3** CNN (input image of 299×299) that maps images to semantic vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/inceptionv3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **intermediate layers** of the CNN learn **local features** of different **sub-regions of the image**, while the later layers learn global features of the image.    \n",
    "\n",
    "We extract the **local feature** matrix **f** ∈ R768⇥289 (reshaped from 768×17×17, 17x17=289) from the **“mixed 6e” layer** of Inception-v3.  \n",
    "Each column of **f** is the **feature vector** of a **sub-region of the image**.  \n",
    "\n",
    "**f** shape is (768, 289):  \n",
    "768 is the dimension of the local feature vector, and  \n",
    "289 is the number of sub-regions in the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_ENCODER(nn.Module):\n",
    "    def __init__(self, nef):\n",
    "        ''' \n",
    "        nef <-- TEXT.EMBEDDING_DIM = 256 (does 'nef' stands for 'Number Embedding Features'?)\n",
    "        '''\n",
    "        super(CNN_ENCODER, self).__init__()\n",
    "        if cfg.TRAIN.FLAG:\n",
    "            self.nef = nef\n",
    "        else:\n",
    "            self.nef = 256  # define a uniform ranker\n",
    "\n",
    "        model = models.inception_v3()\n",
    "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "        model.load_state_dict(model_zoo.load_url(url))\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.define_module(model)\n",
    "        self.init_trainable_weights()\n",
    "\n",
    "    def define_module(self, model):\n",
    "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
    "        # ...\n",
    "        self.Mixed_5d = model.Mixed_5d\n",
    "        self.Mixed_6a = model.Mixed_6a\n",
    "        self.Mixed_6b = model.Mixed_6b\n",
    "        # ...\n",
    "        self.emb_features = conv1x1(768, self.nef)\n",
    "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
    "\n",
    "    def init_trainable_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
    "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = None\n",
    "        # --> fixed-size input: batch x 3 x 299 x 299\n",
    "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
    "        # 299 x 299 x 3\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # 149 x 149 x 32\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "\n",
    "        # ...\n",
    "\n",
    "        x = self.Mixed_6e(x)\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        # --- image region features ---\n",
    "        features = x\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        x = self.Mixed_7a(x)\n",
    "        # 8 x 8 x 1280\n",
    "        x = self.Mixed_7b(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = self.Mixed_7c(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = F.avg_pool2d(x, kernel_size=8)\n",
    "        # 1 x 1 x 2048\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # 2048\n",
    "\n",
    "        # --- global image features ---\n",
    "        cnn_code = self.emb_cnn_code(x)             # self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
    "        # 512\n",
    "        if features is not None:\n",
    "            features = self.emb_features(features)  # self.emb_features = conv1x1(768, self.nef)\n",
    "\n",
    "        return features, cnn_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_ENCODER(\n",
    "  (Conv2d_1a_3x3): BasicConv2d(\n",
    "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  )\n",
    "  (Conv2d_2a_3x3): BasicConv2d(\n",
    "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  )\n",
    "  (Conv2d_2b_3x3): BasicConv2d(\n",
    "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  )\n",
    "  (Conv2d_3b_1x1): BasicConv2d(\n",
    "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  )\n",
    "  (Conv2d_4a_3x3): BasicConv2d(\n",
    "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  )\n",
    "  (Mixed_5b): InceptionA(\n",
    "    (branch1x1): BasicConv2d(\n",
    "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch5x5_1): BasicConv2d(\n",
    "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch5x5_2): BasicConv2d(\n",
    "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
    "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch3x3dbl_1): BasicConv2d(\n",
    "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch3x3dbl_2): BasicConv2d(\n",
    "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch3x3dbl_3): BasicConv2d(\n",
    "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch_pool): BasicConv2d(\n",
    "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "  )\n",
    "\n",
    "...\n",
    "    \n",
    "  (Mixed_6e): InceptionC(\n",
    "    (branch1x1): BasicConv2d(\n",
    "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch7x7_1): BasicConv2d(\n",
    "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch7x7_2): BasicConv2d(\n",
    "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch7x7_3): BasicConv2d(\n",
    "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch7x7dbl_1): BasicConv2d(\n",
    "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch7x7dbl_2): BasicConv2d(\n",
    "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch7x7dbl_3): BasicConv2d(\n",
    "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch7x7dbl_4): BasicConv2d(\n",
    "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch7x7dbl_5): BasicConv2d(\n",
    "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch_pool): BasicConv2d(\n",
    "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "  )\n",
    "    \n",
    "...\n",
    "    \n",
    "  (Mixed_7c): InceptionE(\n",
    "    (branch1x1): BasicConv2d(\n",
    "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch3x3_1): BasicConv2d(\n",
    "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch3x3_2a): BasicConv2d(\n",
    "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
    "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch3x3_2b): BasicConv2d(\n",
    "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
    "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch3x3dbl_1): BasicConv2d(\n",
    "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch3x3dbl_2): BasicConv2d(\n",
    "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch3x3dbl_3a): BasicConv2d(\n",
    "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
    "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch3x3dbl_3b): BasicConv2d(\n",
    "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
    "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "    (branch_pool): BasicConv2d(\n",
    "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    )\n",
    "  )\n",
    "  (emb_features): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "  (emb_cnn_code): Linear(in_features=2048, out_features=256, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.TEXT.EMBEDDING_DIM = 256\n",
    "dataset.n_words = 2932   # for FashionGen subset. Computed by dataset.load_text_data(): parsing all captions\n",
    "\n",
    "def build_models():\n",
    "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    batch_size = 32\n",
    "\n",
    "    for step, data in enumerate(dataloader, 0):\n",
    "        rnn_model.zero_grad()\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
    "\n",
    "        #\n",
    "        # imgs -- list of 1 image tensor\n",
    "        # imgs[0].shape --> torch.Size([32, 3, 299, 299])\n",
    "        #\n",
    "        # cap_lens.shape(32) -->     tensor([10, 10, 10, 10, 10, 10, 10,  9,  9,  8,  8,  8,  7,  7,  7,  7,  \n",
    "        #                                     7,  7,  7,  7,  7,  6,  6,  6,  6,  6,  6,  6,  5,  5,  5,  5]\n",
    "        # \n",
    "        #\n",
    "        # captions.shape(32, 10) --> tensor([[1151,   16,  526,  241, 1240, 1944, 1443,  303,  526, 1147],\n",
    "        #                                    [2331,  195, 1624, 1151, 1078, 2859, 1837,   16,  526, 1147],\n",
    "        #                                     ...\n",
    "        #                                    [2153, 1837, 2538,  526, 1147,    0,    0,    0,    0,    0]]\n",
    "        #\n",
    "        # nef -- cfg.TEXT.EMBEDDING_DIM = 256 (for FashionGen)\n",
    "\n",
    "        # words_features: batch_size x nef x 17 x 17\n",
    "        # sentence_feature: batch_size x nef\n",
    "        words_features, sentence_feature = cnn_model(imgs[-1])\n",
    "        \n",
    "        # words_features.shape   --> torch.Size([32, 256, 17, 17])\n",
    "        # sentence_feature.shape --> torch.Size([32, 256])\n",
    "        \n",
    "        # --> batch_size x nef x 17*17\n",
    "        nef, att_sze = words_features.size(1), words_features.size(2)\n",
    "        \n",
    "        # nef -> 256\n",
    "        # att_sze -> 17\n",
    "        # words_features = words_features.view(batch_size, nef, -1)\n",
    "\n",
    "        hidden = rnn_model.init_hidden(batch_size)\n",
    "        \n",
    "        # hidden -> list of size 2\n",
    "        # hidden[0].shape -> torch.Size([2, 32, 128]) -- all zeros\n",
    "        # hidden[1].shape -> torch.Size([2, 32, 128]) -- all zeros\n",
    "        \n",
    "        # words_emb: batch_size x nef x seq_len\n",
    "        # sent_emb: batch_size x nef\n",
    "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
    "\n",
    "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels, cap_lens, class_ids, batch_size)\n",
    "        w_total_loss0 += w_loss0.data\n",
    "        w_total_loss1 += w_loss1.data\n",
    "        loss = w_loss0 + w_loss1\n",
    "\n",
    "        s_loss0, s_loss1 = sent_loss(sentence_feature, sent_emb, labels, class_ids, batch_size)\n",
    "        loss += s_loss0 + s_loss1\n",
    "        s_total_loss0 += s_loss0.data\n",
    "        s_total_loss1 += s_loss1.data\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), cfg.TRAIN.RNN_GRAD_CLIP)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
